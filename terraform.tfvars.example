# Example terraform.tfvars file
# Copy this to terraform.tfvars and modify as needed

# Cluster configuration
region         = "us-east-1"
cluster_name   = "eks-prod"
cluster_version = "1.29"

# IAM role configuration
# In environments with IAM restrictions, set to false and provide existing roles
create_cluster_iam_role = false
# Specify existing cluster role if create_cluster_iam_role = false
# This is the IAM role that the EKS service itself will use
cluster_iam_role_arn = "arn:aws:iam::123456789012:role/my-existing-eks-cluster-role"

# Node IAM role configuration
create_node_iam_role = false
# Specify existing node role if create_node_iam_role = false
# This is the IAM role that all worker nodes will use
node_iam_role_arn = "arn:aws:iam::123456789012:role/my-existing-eks-node-role"

# VPC configuration (use existing VPC)
vpc_mode      = "existing"
vpc_id        = "vpc-01234567890abcdef"
subnet_ids    = ["subnet-01234567890abcdef", "subnet-01234567890abcdef", "subnet-01234567890abcdef"]
control_plane_subnet_ids = ["subnet-01234567890abcdef", "subnet-01234567890abcdef", "subnet-01234567890abcdef"]

# Node group configuration
eks_managed_node_groups = {
  default = {
    name = "default-node-group"
    instance_types = ["m5.large"]
    capacity_type  = "ON_DEMAND"
    min_size     = 2
    max_size     = 5
    desired_size = 2
    labels = {
      Environment = "prod"
      Role        = "general"
    }
    # Custom AMI for this specific node group - will use launch template
    ami_id = "ami-0123456789abcdef0"
    
    # Optional: Additional bootstrap arguments for custom AMI
    # bootstrap_extra_args = "--use-max-pods false"
    
    # Optional: Additional kubelet arguments
    # kubelet_extra_args = "--node-labels=node.kubernetes.io/workload-type=cpu"
    
    # EKS 1.29+ supports node update configuration to minimize disruption
    # update_config = {
    #   max_unavailable_percentage = 33  # OR max_unavailable = 1
    # }
    
    # EKS 1.29+ supports node repair configuration
    # node_repair_config = {
    #   enabled = true
    # }
  }
  
  # Example of a second node group with different custom AMI
  special = {
    name = "special-node-group"
    instance_types = ["c5.large"]
    capacity_type  = "ON_DEMAND"
    min_size     = 1
    max_size     = 3
    desired_size = 1
    labels = {
      Environment = "prod"
      Role        = "special"
    }
    ami_id = "ami-0987654321fedcba0"  # Different custom AMI
  }
}

# Optional: Custom AMI ID for all node groups (can be overridden per node group)
node_group_ami_id = "ami-0123456789abcdef0"

# Custom AMI usage:
# To use custom AMIs with your EKS node groups, simply specify the ami_id in your node groups:
# eks_managed_node_groups = {
#   custom-group = {
#     ami_id = "ami-0123456789abcdef0"
#     # When using ami_id with custom launch templates
#     
#     # Block device mappings - REQUIRED for organizational policies
#     # Must use gp3 (not gp2) and enable encryption
#     block_device_mappings = {
#       root = {
#         device_name = "/dev/xvda"
#         ebs = {
#           volume_size           = 50
#           volume_type           = "gp3"     # REQUIRED: Must use gp3 instead of gp2
#           iops                  = 3000
#           throughput            = 150
#           encrypted             = true      # REQUIRED: Must be encrypted
#           delete_on_termination = true
#         }
#       }
#     }
#     ...
#   }
# }
#
# The module will automatically:
# 1. Create custom launch templates with the specified AMI
# 2. Configure user data with the right bootstrap scripts
# 3. Set proper tags and metadata configuration
# 4. Configure volumes to use gp3 (not gp2) and enable encryption
#
# This approach uses custom launch templates that YOU control
# Your node IAM role needs the proper EC2 permissions.
#
# ADVANCED USAGE (RARELY NEEDED): Pre-created launch templates
# Most users should NOT need this - the default approach above is simpler and more reliable.
# Only use this if you have specific requirements for pre-created launch templates.
#
# To use pre-created launch templates instead of letting EKS create them:
use_existing_launch_templates = false  # Change to true only if using pre-created templates
# Map node group names to their pre-created launch template ARNs:
launch_template_arns = {
  # For example:
  # "my-custom-node-group" = "arn:aws:ec2:us-east-1:123456789012:launch-template/lt-0123456789abcdef"
}

# Cluster networking configuration
service_ipv4_cidr = "172.20.0.0/16"  # Kubernetes service CIDR
cluster_ip_family = "ipv4"           # IP family (ipv4 or ipv6)

# Access configuration - new in EKS module v20 (Optional)
# eks_access_entries = {
#   # Example role with admin access
#   admin-role = {
#     principal_arn = "arn:aws:iam::123456789012:role/eks-admin"
#     policy_associations = {
#       admin = {
#         policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
#         access_scope = {
#           type = "cluster"
#         }
#       }
#     }
#   }
# }

# Add-ons configuration
enable_aws_load_balancer_controller = true
node_scaling_method                 = "karpenter"  # "karpenter", "cluster_autoscaler", or "none"
enable_keda                         = true
enable_external_dns                 = true
enable_prometheus                   = true
enable_secrets_manager              = true
enable_cert_manager                 = true
enable_nginx_ingress                = true
enable_adot                         = true
enable_fluent_bit                   = true
enable_ebs_csi_driver               = true
enable_efs_csi_driver               = false

# External DNS configuration (if enabled)
external_dns_hosted_zone_source     = "existing"  # "existing" or "create"
external_dns_existing_hosted_zone_id = "Z1234567890ABCDEFGHIJK"  # Required if external_dns_hosted_zone_source is "existing"
external_dns_domain                 = "example.com"  # Required if external_dns_hosted_zone_source is "create"

# GitLab integration (for Kubernetes components installation)
trigger_gitlab_pipeline   = true   # Set to true to enable GitLab CI/CD integration for Helm charts

# GitLab AWS Authentication
# You must provide the GitLab role ARN for GitLab integration
gitlab_aws_role_arn = "arn:aws:iam::123456789012:role/MyGitLabDeploymentRole"

# Organization policy required tag
component_id = "aws-eks-cluster"  # Required by organizational policy "DenyWithNoCompTag"
component_id_enabled = true       # Set to false to disable the ComponentID tag if needed

# IAM Admin Role for Add-on IAM Resources
# Specify a role ARN with permissions to create IAM roles and policies
# This role will be used exclusively for creating add-on IAM resources when 
# your GitLab runner role has an explicit deny for IAM role creation
# How it works:
# 1. The module will create an AWS provider alias that assumes this role
# 2. All IAM resources in add-on modules will use this provider
# 3. Regular resources continue to use the default provider (GitLab runner role)
# 
# The role should have these permissions:
# - iam:CreateRole
# - iam:PutRolePolicy
# - iam:CreatePolicy 
# - iam:AttachRolePolicy
# 
# Example policy for this role:
# {
#   "Version": "2012-10-17",
#   "Statement": [
#     {
#       "Effect": "Allow",
#       "Action": [
#         "iam:CreateRole",
#         "iam:DeleteRole",
#         "iam:PutRolePolicy",
#         "iam:DeleteRolePolicy",
#         "iam:CreatePolicy",
#         "iam:DeletePolicy",
#         "iam:AttachRolePolicy",
#         "iam:DetachRolePolicy",
#         "iam:TagRole",
#         "iam:TagPolicy"
#       ],
#       "Resource": "*",
#       "Condition": {
#         "StringEquals": {
#           "aws:ResourceTag/ManagedBy": "terraform"
#         }
#       }
#     }
#   ]
# }
iam_admin_role_arn = "arn:aws:iam::123456789012:role/IAMAdminRole"

# IAM role creation control
# Set to false if you have organizations that restrict IAM role creation
# and need to use pre-existing roles for add-ons
create_addon_roles = true

# Pre-existing IAM roles (only used when create_addon_roles = false)
# Map of add-on names to their existing IAM role ARNs
addon_role_arns = {
  # aws_load_balancer_controller = "arn:aws:iam::123456789012:role/existing-aws-lb-controller-role"
  # karpenter                    = "arn:aws:iam::123456789012:role/existing-karpenter-role"
  # cluster_autoscaler           = "arn:aws:iam::123456789012:role/existing-cluster-autoscaler-role"
  # keda                         = "arn:aws:iam::123456789012:role/existing-keda-role"
  # external_dns                 = "arn:aws:iam::123456789012:role/existing-external-dns-role"
  # prometheus                   = "arn:aws:iam::123456789012:role/existing-prometheus-role"
  # secrets_manager              = "arn:aws:iam::123456789012:role/existing-secrets-manager-role"
  # cert_manager                 = "arn:aws:iam::123456789012:role/existing-cert-manager-role"
  # nginx_ingress                = "arn:aws:iam::123456789012:role/existing-nginx-ingress-role"
  # adot                         = "arn:aws:iam::123456789012:role/existing-adot-role"
  # fluent_bit                   = "arn:aws:iam::123456789012:role/existing-fluent-bit-role"
  # ebs_csi_driver               = "arn:aws:iam::123456789012:role/existing-ebs-csi-driver-role"
  # efs_csi_driver               = "arn:aws:iam::123456789012:role/existing-efs-csi-driver-role"
}

# Pre-existing IAM role names (only used when create_addon_roles = false)
# Map of add-on names to their existing IAM role names - optional
addon_role_names = {
  # aws_load_balancer_controller = "existing-aws-lb-controller-role"
  # karpenter                    = "existing-karpenter-role"
}

# Tags
tags = {
  Environment = "production"
  ManagedBy   = "terraform"
  Project     = "eks-cluster"
}