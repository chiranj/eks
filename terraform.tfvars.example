# Example terraform.tfvars file
# Copy this to terraform.tfvars and modify as needed

# Cluster configuration
region         = "us-east-1"
cluster_name   = "eks-prod"
cluster_version = "1.29"

# IAM role configuration
# In environments with IAM restrictions, set to false and provide an existing role
create_cluster_iam_role = false
# Specify existing cluster role if create_cluster_iam_role = false
# This is the IAM role that the EKS service itself will use
cluster_iam_role_arn = "arn:aws:iam::123456789012:role/my-existing-eks-cluster-role"

# VPC configuration (use existing VPC)
vpc_mode      = "existing"
vpc_id        = "vpc-01234567890abcdef"
subnet_ids    = ["subnet-01234567890abcdef", "subnet-01234567890abcdef", "subnet-01234567890abcdef"]
control_plane_subnet_ids = ["subnet-01234567890abcdef", "subnet-01234567890abcdef", "subnet-01234567890abcdef"]

# Node group configuration
eks_managed_node_groups = {
  default = {
    name = "default-node-group"
    instance_types = ["m5.large"]
    capacity_type  = "ON_DEMAND"
    min_size     = 2
    max_size     = 5
    desired_size = 2
    labels = {
      Environment = "prod"
      Role        = "general"
    }
    # Custom AMI for this specific node group - will use launch template
    ami_id = "ami-0123456789abcdef0"
    
    # Optional: Additional bootstrap arguments for custom AMI
    # bootstrap_extra_args = "--use-max-pods false"
    
    # Optional: Additional kubelet arguments
    # kubelet_extra_args = "--node-labels=node.kubernetes.io/workload-type=cpu"
    
    # EKS 1.29+ supports node update configuration to minimize disruption
    # update_config = {
    #   max_unavailable_percentage = 33  # OR max_unavailable = 1
    # }
    
    # EKS 1.29+ supports node repair configuration
    # node_repair_config = {
    #   enabled = true
    # }
  }
  
  # Example of a second node group with different custom AMI
  special = {
    name = "special-node-group"
    instance_types = ["c5.large"]
    capacity_type  = "ON_DEMAND"
    min_size     = 1
    max_size     = 3
    desired_size = 1
    labels = {
      Environment = "prod"
      Role        = "special"
    }
    ami_id = "ami-0987654321fedcba0"  # Different custom AMI
  }
}

# Optional: Custom AMI ID for all node groups (can be overridden per node group)
node_group_ami_id = "ami-0123456789abcdef0"

# Launch template configuration for custom AMIs
# Now fixed to properly handle type inconsistency
create_launch_templates_for_custom_amis = true

# Cluster networking configuration
service_ipv4_cidr = "172.20.0.0/16"  # Kubernetes service CIDR
cluster_ip_family = "ipv4"           # IP family (ipv4 or ipv6)

# Access configuration - new in EKS module v20 (Optional)
# eks_access_entries = {
#   # Example role with admin access
#   admin-role = {
#     principal_arn = "arn:aws:iam::123456789012:role/eks-admin"
#     policy_associations = {
#       admin = {
#         policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
#         access_scope = {
#           type = "cluster"
#         }
#       }
#     }
#   }
# }

# Add-ons configuration
enable_aws_load_balancer_controller = true
node_scaling_method                 = "karpenter"  # "karpenter", "cluster_autoscaler", or "none"
enable_keda                         = true
enable_external_dns                 = true
enable_prometheus                   = true
enable_secrets_manager              = true
enable_cert_manager                 = true
enable_nginx_ingress                = true
enable_adot                         = true
enable_fluent_bit                   = true
enable_ebs_csi_driver               = true
enable_efs_csi_driver               = false

# External DNS configuration (if enabled)
external_dns_hosted_zone_source     = "existing"  # "existing" or "create"
external_dns_existing_hosted_zone_id = "Z1234567890ABCDEFGHIJK"  # Required if external_dns_hosted_zone_source is "existing"
external_dns_domain                 = "example.com"  # Required if external_dns_hosted_zone_source is "create"

# GitLab integration (for Kubernetes components installation)
trigger_gitlab_pipeline   = true   # Set to true to enable GitLab CI/CD integration for Helm charts

# GitLab AWS Authentication
# You must provide the GitLab role ARN for GitLab integration
gitlab_aws_role_arn = "arn:aws:iam::123456789012:role/MyGitLabDeploymentRole"

# Tags
tags = {
  Environment = "production"
  ManagedBy   = "terraform"
  Project     = "eks-cluster"
}